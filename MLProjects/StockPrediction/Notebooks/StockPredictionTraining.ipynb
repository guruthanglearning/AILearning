{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    " # Creates shared session to reduce some rate limit\n",
    "\n",
    "symbols = ['AAPL']\n",
    "\n",
    "for symbol in symbols:    \n",
    "    session = requests.Session()    \n",
    "    df = yf.download(symbol, session=session, period=\"1y\", interval=\"1d\", auto_adjust=True, group_by=\"column\")\n",
    "\n",
    "    # Reset index to flatten DataFrame and avoid multi-index issues\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Remove any unnecessary multi-index or redundant columns if they exist\n",
    "    \n",
    "    if 'Ticker' in df.columns:\n",
    "        df = df.drop(columns=['Ticker'])\n",
    "    \n",
    "    rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "    directoryPath = os.path.join(rootPath, \"Data\",symbol)\n",
    "    # Save the cleaned DataFrame correctly\n",
    "    if not os.path.exists(directoryPath):\n",
    "        os.makedirs(directoryPath, exist_ok=True)\n",
    "\n",
    "    filePath = os.path.join(directoryPath, f\"stock_data_{symbol}.csv\")   \n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath)    \n",
    "\n",
    "    # Save CSV with proper index label\n",
    "    df.to_csv(filePath, index=False)\n",
    "    \n",
    "    print(f\"Data for {symbol} saved\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 238.35, 'd': 0.32, 'dp': 0.1344, 'h': 240.07, 'l': 236.16, 'o': 237.705, 'pc': 238.03, 't': 1741118710}\n"
     ]
    }
   ],
   "source": [
    "# Gets stock data from finnhub\n",
    "import finnhub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "df = finnhub_client.quote('AAPL')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Total Current Assets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Total Current Assets'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     10\u001b[0m  \u001b[38;5;66;03m# Extract last 4 quarters of data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m last_4_quarters \u001b[38;5;241m=\u001b[39m balance_sheet\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m: symbol,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarket_cap\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarketCap\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpe_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailingPE\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrailingEps\u001b[39m\u001b[38;5;124m\"\u001b[39m),    \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdividend_yield\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdividendYield\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: stock\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbalance_sheet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotal Current Assets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_4_quarters\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_liabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m: balance_sheet\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Current Liabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m, last_4_quarters]\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperating_income\u001b[39m\u001b[38;5;124m\"\u001b[39m: income_statement\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperating Income\u001b[39m\u001b[38;5;124m\"\u001b[39m, last_4_quarters]\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m: income_statement\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m, last_4_quarters]\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfree_cash_flow\u001b[39m\u001b[38;5;124m\"\u001b[39m: cashflow_statement\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFree Cash Flow\u001b[39m\u001b[38;5;124m\"\u001b[39m, last_4_quarters]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInfo: \u001b[39m\u001b[38;5;132;01m{info}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m (json\u001b[38;5;241m.\u001b[39mdumps(result,indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\generic.py:4301\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4299\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4301\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[1;32md:\\Study\\AILearning\\shared_Environment\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Total Current Assets'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import json\n",
    "symbol = 'AAPL'\n",
    "stock = yf.Ticker(symbol)\n",
    "info = stock.info\n",
    "# Fetch financial statements\n",
    "balance_sheet = stock.balance_sheet\n",
    "income_statement = stock.financials\n",
    "cashflow_statement = stock.cashflow\n",
    " # Extract last 4 quarters of data\n",
    "last_4_quarters = balance_sheet.columns[:4]\n",
    "\n",
    "def safe_get(df, key, columns):\n",
    "    \"\"\"Safely get the value from DataFrame if the key exists.\"\"\"\n",
    "    if key in df.index:\n",
    "        return df.loc[key, columns].to_dict()\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "result = {\n",
    "    \"symbol\": symbol,\n",
    "    \"market_cap\": info.get(\"marketCap\"),\n",
    "    \"pe_ratio\": info.get(\"trailingPE\"),\n",
    "    \"eps\": info.get(\"trailingEps\"),    \n",
    "    \"dividend_yield\": info.get(\"dividendYield\"),\n",
    "    \"sector\": info.get(\"sector\"),\n",
    "    \"industry\": info.get(\"industry\"),\n",
    "    \"beta\": stock.info.get(\"beta\"),\n",
    "    \"current_assets\": safe_get(balance_sheet, \"Total Current Assets\", last_4_quarters),\n",
    "    \"current_liabilities\": safe_get(balance_sheet, \"Total Current Liabilities\", last_4_quarters),\n",
    "    \"operating_income\": safe_get(income_statement, \"Operating Income\", last_4_quarters),\n",
    "    \"total_revenue\": safe_get(income_statement, \"Total Revenue\", last_4_quarters),\n",
    "    \"free_cash_flow\": safe_get(cashflow_statement, \"Free Cash Flow\", last_4_quarters)\n",
    "}\n",
    "\n",
    "print(\"Info: {info}\")\n",
    "print(json.dumps(result, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame and display stock information\n",
    "import pandas as pd\n",
    "res = pd.DataFrame(list(df.items()), columns=['Metrics', 'Values'])\n",
    "display(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading AAPL data from local file\n",
    "import pandas as pd\n",
    "symbol = 'AAPL'\n",
    "rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "directoryPath = os.path.join(rootPath, \"Data\",symbol)\n",
    "filePath = os.path.join(directoryPath, f\"stock_data_{symbol}.csv\")   \n",
    "df = pd.read_csv(filePath,skiprows=[1])\n",
    "print(df.columns.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating technical indicators\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta  # Technical Analysis library for indicators\n",
    "import os\n",
    "\n",
    "def custom_zigzag(prices, threshold=0.05):\n",
    "    \"\"\"Custom ZigZag indicator that identifies significant price reversals.\"\"\"\n",
    "    trend = np.zeros(len(prices))\n",
    "    last_pivot = prices[0]\n",
    "\n",
    "    for i in range(1, len(prices)):\n",
    "        change = (prices[i] - last_pivot) / last_pivot\n",
    "        if abs(change) >= threshold:  # Significant movement detected\n",
    "            trend[i] = prices[i]\n",
    "            last_pivot = prices[i]\n",
    "        else:\n",
    "            trend[i] = np.nan  # Minor fluctuations ignored\n",
    "    return pd.Series(trend)\n",
    "\n",
    "\n",
    "# Ensure 'Date' column is datetime type and set as index\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "# Calculate Simple Moving Averages (SMA)\n",
    "df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
    "df['SMA_200'] = ta.trend.sma_indicator(df['Close'], window=200)\n",
    "\n",
    "# Calculate Exponential Moving Averages (EMA)\n",
    "df['EMA_20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
    "df['EMA_200'] = ta.trend.ema_indicator(df['Close'], window=200)\n",
    "\n",
    "# Calculate Relative Strength Index (RSI)\n",
    "df['RSI_7'] = ta.momentum.rsi(df['Close'], window=7)\n",
    "df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "df['RSI_200'] = ta.momentum.rsi(df['Close'], window=200)\n",
    "\n",
    "# Calculate MACD (Moving Average Convergence Divergence)\n",
    "df['MACD'] = ta.trend.macd(df['Close'], window_slow=13, window_fast=6)\n",
    "\n",
    "# Calculate Average True Range (ATR)\n",
    "df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=14)\n",
    "\n",
    "# Apply Custom ZigZag Trend Detection\n",
    "df['ZigZag'] = custom_zigzag(df['Close'].values, threshold=0.05)\n",
    "\n",
    "# Save processed data with technical indicators\n",
    "rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "directoryPath = os.path.join(rootPath, \"Data\",symbol)\n",
    "outputPath = os.path.join(directoryPath, f\"technical_indicators_{symbol}.csv\")   \n",
    "if os.path.exists(outputPath):\n",
    "        os.remove(outputPath)\n",
    "df.to_csv(outputPath)\n",
    "\n",
    "print(f\"Technical indicators for {symbol} saved to {outputPath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Forecasting Model:\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "    \n",
    "\"\"\"Train an XGBoost regression model to forecast stock prices (Close, High, Low).\"\"\"\n",
    "# Load technical indicators from CSV\n",
    "rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "directoryPath = os.path.join(rootPath, \"Data\",symbol)\n",
    "filePath = os.path.join(directoryPath, f\"technical_indicators_{symbol}.csv\")   \n",
    "\n",
    "df = pd.read_csv(filePath, index_col='Date', parse_dates=True)\n",
    "\n",
    "# Prepare features using technical indicators\n",
    "features = df[['SMA_20', 'SMA_200', 'EMA_20', 'EMA_200', 'RSI_7', 'RSI_14', 'RSI_200', 'MACD', 'ATR']]\n",
    "\n",
    "# Define target variables: Predict next day's Close, High, and Low prices\n",
    "target_close = df['Close'].shift(-1)  # Predict next day's close price\n",
    "target_high = df['High'].shift(-1)    # Predict next day's high price\n",
    "target_low = df['Low'].shift(-1)      # Predict next day's low price\n",
    "\n",
    "# Remove rows with missing values\n",
    "valid_rows = ~target_close.isna()\n",
    "features = features[valid_rows]\n",
    "target_close = target_close[valid_rows]\n",
    "target_high = target_high[valid_rows]\n",
    "target_low = target_low[valid_rows]\n",
    "\n",
    "# Check for NaN or infinite values and handle them\n",
    "features = features.fillna(0)\n",
    "features.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Normalize the features for better model performance\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "def train_single_target(y, target_name):\n",
    "    # Split data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and train the XGBoost regressor\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model using Mean Squared Error (MSE)\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error for {symbol} - {target_name}: {mse}\")\n",
    "\n",
    "    # Save the model    \n",
    "    model_dir = os.path.join(rootPath, \"Models\",\"Price_Forecast\",symbol)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filePath = os.path.join(model_dir, f\"Price_Forecast_{symbol}_{target_name}.json\")       \n",
    "    model.save_model(filePath)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train models for each target (Close, High, Low)\n",
    "close_model = train_single_target(target_close, \"close\")\n",
    "high_model = train_single_target(target_high, \"high\")\n",
    "low_model = train_single_target(target_low, \"low\")\n",
    "\n",
    "print(f\"Trained and saved XGBoost regression models for {symbol}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend Classification Model \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "symbol = 'AAPL'\n",
    "\n",
    "rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "directoryPath = os.path.join(rootPath, \"Data\",symbol)\n",
    "\n",
    " # Load technical indicators from CSV\n",
    "filePath = os.path.join(directoryPath, f\"technical_indicators_{symbol}.csv\")  \n",
    "df = pd.read_csv(filePath, index_col='Date', parse_dates=True)\n",
    "\n",
    "# Prepare features using technical indicators\n",
    "features = df[['SMA_20', 'SMA_200', 'EMA_20', 'EMA_200', 'RSI_7', 'RSI_14', 'RSI_200', 'MACD', 'ATR']]\n",
    "\n",
    "# Define target variable: Trend classification\n",
    "df['price_change'] = df['Close'].pct_change().shift(-1)\n",
    "df['trend'] = df['price_change'].apply(lambda x: 1 if x > 0.005 else -1 if x < -0.005 else 0)\n",
    "\n",
    "# Ensure valid rows\n",
    "valid_rows = ~df['trend'].isna()\n",
    "features = features[valid_rows]\n",
    "target_trend = df['trend'][valid_rows]\n",
    "\n",
    "# Handle NaNs and infinite values in features\n",
    "features = features.fillna(0)\n",
    "features.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# Map trend labels to non-negative values for classification\n",
    "y = target_trend.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "# Ensure target labels are integer values\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "print(f\"Classification Report for {symbol}:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(f\"Confusion Matrix for {symbol}:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Save the trained model\n",
    "directoryPath = os.path.join(rootPath, \"Models\", \"Trend_Classification\", symbol)\n",
    "model_dir = os.path.join(directoryPath, f\"Trend_classification_{symbol}.json\")\n",
    "if os.path.exists(model_dir):\n",
    "        os.remove(model_dir)\n",
    "os.makedirs(directoryPath, exist_ok=True)\n",
    "model.save_model(model_dir)\n",
    "\n",
    "print(f\"Trained and saved trend classification model for {symbol}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real time prediction\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta # Technical analysis library\n",
    "\n",
    "\n",
    "symbol = 'AAPL'\n",
    "rootPath =   os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "\n",
    "# Function to compute necessary technical indicators from real-time data\n",
    "def compute_real_time_indicators(df):\n",
    "    \"\"\"Calculate technical indicators from the latest real-time stock data.\"\"\"      \n",
    "\n",
    "    print(df.columns)\n",
    "    # Calculate SMA, EMA, RSI, MACD, ATR\n",
    "    df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
    "    df['SMA_200'] = ta.trend.sma_indicator(df['Close'], window=200)\n",
    "    df['EMA_20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
    "    df['EMA_200'] = ta.trend.ema_indicator(df['Close'], window=200)\n",
    "    df['RSI_7'] = ta.momentum.rsi(df['Close'], window=7)\n",
    "    df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "    df['RSI_200'] = ta.momentum.rsi(df['Close'], window=200)\n",
    "    df['MACD'] = ta.trend.macd(df['Close'], window_slow=13, window_fast=6)\n",
    "    df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=14)\n",
    "\n",
    "    # Select the last row (most recent data) for prediction\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    latest_data = df.iloc[-1][['SMA_20', 'SMA_200', 'EMA_20', 'EMA_200', 'RSI_7', 'RSI_14', 'RSI_200', 'MACD', 'ATR']].fillna(0)\n",
    "    return latest_data.values.reshape(1, -1)  # Reshape for prediction\n",
    "\n",
    "# Function to load a trained XGBoost model\n",
    "def load_model(symbol, target):\n",
    "    \"\"\"Load the trained model for the given symbol and target price.\"\"\"\n",
    "\n",
    "    model_path = os.path.join(rootPath, \"Models\", \"Price_Forecast\", symbol, f\"Price_Forecast_{symbol}_{target}.json\")\n",
    "    if os.path.exists(model_path):\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.load_model(model_path)\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model for {symbol} - {target} not found.\")\n",
    "\n",
    "\n",
    "filePath = os.path.join(rootPath, \"Data\", symbol, f\"stock_data_{symbol}.csv\")\n",
    "\n",
    "if not os.path.exists(filePath):         \n",
    "    df = yf.download(\n",
    "        symbol,\n",
    "        period=\"30d\",  # Fetch the latest 5 days for calculating indicators\n",
    "        interval=\"1d\",  # Daily interval\n",
    "        auto_adjust=True,  # Adjust for dividends and splits\n",
    "        group_by=\"column\"  # Ensure flat data structure\n",
    "    )\n",
    "\n",
    "    # Reset index to flatten Date index and remove multi-index issues\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Remove 'Ticker' column if it exists to avoid redundancy\n",
    "    if 'Ticker' in df.columns:\n",
    "        df = df.drop(columns=['Ticker']) \n",
    "\n",
    "    # Save to CSV with the proper index label\n",
    "    df.to_csv(filePath)     \n",
    "else:\n",
    "    df = pd.read_csv(filePath, skiprows=[1])\n",
    "\n",
    "\"\"\"Fetch real-time data, compute indicators, and predict future prices.\"\"\"\n",
    "indicators = compute_real_time_indicators(df)\n",
    "\n",
    "# Scale features before prediction\n",
    "scaler = MinMaxScaler()\n",
    "indicators_scaled = scaler.fit_transform(indicators)\n",
    "\n",
    "# Load trained models\n",
    "close_model = load_model(symbol, \"close\")\n",
    "high_model = load_model(symbol, \"high\")\n",
    "low_model = load_model(symbol, \"low\")\n",
    "\n",
    "# Make predictions\n",
    "predicted_close = close_model.predict(indicators_scaled)[0]\n",
    "predicted_high = high_model.predict(indicators_scaled)[0]\n",
    "predicted_low = low_model.predict(indicators_scaled)[0]\n",
    "\n",
    "print(f\"Symbol: {symbol}\\nPredicted Close: {round(predicted_close, 2)}\\nPredicted High: {round(predicted_high, 2)}\\nPredicted Low: {round(predicted_low, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '9to5Mac Daily: February 3, 2025 – AAPL earnings, AR projects', 'sentiment_score': 0.9048196859657764}, {'title': 'AAPL climbed 3% on DeepSeek news, as other tech stocks fell', 'sentiment_score': -0.776688490062952}, {'title': 'Home Depot earnings, Fed, consumer confidence: What to Watch', 'sentiment_score': 0.8836971297860146}, {'title': 'After strong earnings, Morgan Stanley inches up AAPL target to $275', 'sentiment_score': -0.9172416217625141}, {'title': 'Third Point boosts its stake in these ‘Magnificent Seven’ stocks — but offloads this one', 'sentiment_score': -0.9093109704554081}, {'title': 'Apple just unveiled a new version of its cheaper iPhone', 'sentiment_score': 0.6541318744421005}, {'title': 'Trump to Apple: Ditch DEI', 'sentiment_score': 0.8662710040807724}, {'title': \"Apple earnings are coming. Here's what to expect\", 'sentiment_score': 0.8794791251420975}, {'title': 'Apple reports record earnings, but misses iPhone estimates', 'sentiment_score': -0.34469354152679443}, {'title': \"Apple might drop a new iPhone tomorrow. Here's what to know\", 'sentiment_score': 0.8578090034425259}]\n"
     ]
    }
   ],
   "source": [
    "# Market Sentiment\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Load FinBERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "query=\"AAPL\"\n",
    "page_size=10\n",
    "\n",
    "\"\"\"Fetch latest news and compute sentiment score.\"\"\"\n",
    "url = f\"https://newsapi.org/v2/everything?q={query}&pageSize={page_size}&apiKey={os.getenv(\"NEWS_API_KEY\")}\"\n",
    "response = requests.get(url)\n",
    "articles = response.json().get(\"articles\", [])\n",
    "\n",
    "news_data = []\n",
    "for article in articles:\n",
    "    text = article[\"title\"] + \" \" + (article[\"description\"] if article[\"description\"] else \"\")\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = probs[0][2].item() - probs[0][0].item()  # Positive - Negative sentiment\n",
    "\n",
    "    news_data.append({\n",
    "        \"title\": article[\"title\"],\n",
    "        \"sentiment_score\": sentiment\n",
    "    })\n",
    "print(news_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared_Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
